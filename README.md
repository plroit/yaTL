# yaTL
Yet-Another-Training-Loop for Natural Language Processing (based on BERT-like models)
This project is aimed to provide a clean working example of training (fine-tuning) 
of neural models for Natural Language Processing. 
The current best-known practices range from incorporating and indulging into a framework such as AllenNLP,
or trying to use directly the example scripts provided with the transformers python library. 

Sometimes we need more flexibility in the training loop code then a library can provide through hooks. 
Moreover, we don't want to deal with the idiosyncracies outside the scope of our research or work project.
This repository is provided as an example code for direct use and extension in your project.
A simple use case for Part-of-Speech classification is provided with sane defaults command line arguments.
You are the master of your own training loop, now.
 
